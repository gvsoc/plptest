#!/usr/bin/env python3

#
# Copyright (C) 2023 ETH Zurich, University of Bologna and GreenWaves Technologies
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#


import argparse
import logging
import os
import sys

import plptest.runner




def run(runner, args):
    runner.run()

def table(runner, args):
    runner.dump_table()

def junit(runner, args):
    runner.dump_junit(args.junit_report_path)

commands = {
  'run'    : ['Run the tests', run],
  'table'  : ['Dump a report using a table', table],
  'junit'  : ['Dump junit report', junit],
}

commandHelp = """Available commands:
"""

for name, cmd in commands.items():
	commandHelp += '  %-10s %s\n' % (name, cmd[0])

parser = argparse.ArgumentParser(description='Run a testset',epilog=commandHelp, formatter_class=argparse.RawDescriptionHelpFormatter)

parser.add_argument('command', metavar='CMD', type=str, nargs='*',
                   help='a command to be executed (see the command help afterwards)')

parser.add_argument("--testset", dest="testset", action="append", default=None, metavar="PATH", help="Path to the testset. Default: %(default)s")
parser.add_argument("--property", dest="properties", action="append", default=[], help="Specifies property")
parser.add_argument('--py-stack', dest='py_stack', action="store_true", help='Show python exception stack.')
parser.add_argument('--verbose', dest='verbose', action="store_true", help='Enable verbose mode.')
parser.add_argument("--threads", dest="threads", default=0, type=int, help="Specify the number of worker threads")
parser.add_argument("--config", dest="config", default='default', help="Specify config name")
parser.add_argument("--load-average", dest="load_average", default=0.9, type=float, help="Specify the system average load that this tool should try to respect, from 0 to 1")
parser.add_argument("--stdout", dest="stdout", action="store_true", help="Dumps test output to stdout")
parser.add_argument("--safe-stdout", dest="safe_stdout", action="store_true", help="Dumps test output to stdout once the test is done")
parser.add_argument("--max-output-len", dest="max_output_len", type=int, default=-1, help="Maximum length of a test output. Default: %(default)s bytes")
parser.add_argument("--max-timeout", dest="max_timeout", default=-1, type=int, help="Sets maximum timeout allowed for a test")
parser.add_argument("--test", dest="test_list", default=None, action="append", help="Specify a test to be run")
parser.add_argument("--skip", dest="test_skip_list", default=None, action="append", help="Specify a test to be skipped")
parser.add_argument("--cmd", dest="commands", action="append", default=None, metavar="PATH", help="Add command to be executed. Default: %(default)s")
parser.add_argument("--cmd-exclude", dest="commands_exclude", action="append", default=None, metavar="PATH", help="Add command to be excluded. Default: %(default)s")
parser.add_argument("--flags", dest="flags", action="append", default=[], help="Specifies flags")
parser.add_argument("--junit-report-path", dest="junit_report_path", default='junit-reports', help="Specifies flags")
parser.add_argument("--bench-regexp", dest="bench_regexp", default='.*@BENCH@(.*)@DESC@(.*)@', help="Specify regexp for extracting benchmark results")
parser.add_argument("--bench-csv-file", dest="bench_csv_file", default=None, help="Specify CSV file for benchmark results")


args = parser.parse_args()

if args.verbose:
    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')

if len(args.command) == 0:
  args.command.append('run')
  args.command.append('table')

if args.testset is None:
  args.testset = [os.getcwd() + '/testset.cfg']

runner = None

try:
    runner = plptest.runner.Runner(
        config=args.config, load_average=args.load_average, nb_threads=args.threads,
        properties=args.properties, stdout=args.stdout, safe_stdout=args.safe_stdout,
        max_output_len=args.max_output_len, max_timeout=args.max_timeout, test_list=args.test_list,
        test_skip_list=args.test_skip_list, commands=args.commands, commands_exclude=args.commands_exclude,
        flags=args.flags, bench_csv_file=args.bench_csv_file, bench_regexp=args.bench_regexp
    )

    for testset in args.testset:
        runner.add_testset(testset)

    runner.start()

    for command in args.command:
        if commands.get(command) == None:
            raise RuntimeError('Invalid command: ' + command)

        commands.get(command)[1](runner, args)

    runner.stop()

except RuntimeError as e:
    runner.stop()

    if args.py_stack:
        raise

    print('Input error: ' + str(e), file = sys.stderr)
    sys.exit(1)

except:
    if runner is not None:
        runner.stop()
    raise